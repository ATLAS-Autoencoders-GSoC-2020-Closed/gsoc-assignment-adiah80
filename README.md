# HEP-Autoencoders


## INTRODUCTION
- Three models have been trained for the compression task. 
- Each model is trained on `data/all_jets_train_4D_100_percent.pkl` and then tested on `data/all_jets_test_4D_100_percent.pkl`.
- The model code can be accesed by the `.ipynb` files under the respective folders.
- Graphs and model weights for all models are also present in their respective folders.
- Weights are annotated by the validation loss corresponding to the weight.
- While `model1` and `model2` are extentions on the code provided in `https://github.com/Skelpdar/HEPAutoencoders`, `model3` is a pure PyTorch model with a custom training loop.
- An excel sheet containing the results can be found [here](https://docs.google.com/spreadsheets/d/1D71hZTKaa5fwwv7WLjNoa2PCfxveWOJXC6vtx8dnU0E/edit?usp=sharing).
- The accompanying slides can be found [here](https://docs.google.com/presentation/d/1FZjq00JJDuq6j75mJGUELoO2rRWs5uQMkeyyki1iXfg/edit?usp=sharing).

## GRAPHS 
* The following graphs have been included for each model: 
  - Graphs of the input-output distributions for each of the four variables. 
  - Graphs of the residuals (relative difference between output and input) for each of the four variables.
  - Three Graphs of correlation between the variables, one each for three different quantiles.
  - Train and Validation loss plots.
* While the first three plots were included in the initial GitHub repository and have only been slightly improved, the fourth class of plots were implemented from scratch.
* The weight-mean and weight-std-dev plots generated by the fast.ai API were ommited due to multiple training cycles.
* Note: Three graphs were generated for correlation between variables as the original quantile values did not correspond well to the current dataset and had to be reduced. 




## WEIGHTS
* Weigths that were obtained after trianing have been included in the `model` folder under each model.
* There are two kinds of weights : 
  - Files containing the keyword `weight` correspond to weights of the models themselves. 
  - Files containing the keyword `dict` correspond to pickled training logs that are necessary for generating some graphs.
* Functions to load/save the weights have been included in the respective .ipynb notebooks.
* Weights are annotated by the training validation losses, hence it is best to pick the weight with the lowest loss.
* Weights can be chosen using the `val_loss` parameter, which by default is set to the most optimised validation loss.

## RESULTS - COMPRESSION
* Best results were obtained using `model2` with the `AE_3D_100` architecture. 
* These results were obtained when the model was trained in accordance with the strategy suggested by Mr. Eric Wulff in his [thesis](Eric_Wulff_Master_Thesis_v2.pdf) on page 20 : 
```
  [1] Start with a small learning rate to get the weights moving in the right direction.
  [2] Train at a relatively high learning rate for a few epochs to quickly reduce error.
  [3] Finally train for a very long time with a low learning rate.
```
* The above approach led to a validation loss of `0.009958` over the whole test set, which could have been further imporved by training for a few more epochs at a learning rate close to `1e-6`.
* While this loss is significantly larger than the loss obtained in the above thesis (which is `5.2463eâˆ’7`), the difference can atleast partially be attributed to scarsity of training data and fewer training epochs.

## RESULTS - ANOMALY DETECTION
* All the models were tested on a random out of distribution input vector to check whether the trained model had anomaly detection capabilities.
* As expected, the model had a significantly higher (ofter by more than a factor of 10) reconstruction loss for out of distribution inputs compared to standard dataset inputs.
* This hints that the trained model is capable of detecting inputs that do not belong to the underlying distribution.
* The high reconstruction loss for these inputs also suggests the model is successfully using the underlying distribution properties to compress the data.
* Anomaly detection ratios for different models were : 
  - model1 : `2.4273707649797918 / 0.272973702604586   ~ 8.892`
  - model2 : `1.5174107872193567 / 0.0563923950689783  ~ 26.908`
  - model3 : `1.649879813194275  / 0.06075214222073555 ~ 27.157`
* These ratios while calculated on noisy samples, somewhat reinforce the model performance metrics seen in the excel sheet above with model2 and model3 outperforming model1.
